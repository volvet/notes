# 生成模型

## PixelCNN/PixelRNN

## VAE

### KL距离(Kullback-Leibler Divergence)
KL距离的定义:
$$
D(P || Q) = \sum_{x \in X} P(x) \mathop{log} \frac{P(x)}{Q(x)}
$$

正态分布:
$$
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$
$$
E(X) = \mu \\\\
D(X) = \sigma^2 \\\\
E(X^2) = E(X) + D(X) = \mu + \sigma^2
$$


## GAN

## Reference
* 北邮 机器视觉 - 鲁鹏
